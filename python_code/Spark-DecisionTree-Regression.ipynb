{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer,VectorIndexer,VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor,GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator,TrainValidationSplit,ParamGridBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/home/hduser/anaconda3/bin/python'\n",
    "# The value is the python command of the version required to start the master and worker in the Linux system\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\billy\\anaconda3\\python.exe\"\n",
    "# The value is the spark directory in the local windows system\n",
    "os.environ['SPARK_HOME'] = 'C:/spark'\n",
    "# The value is the local IP, and the IP required to establish a connection, to prevent connection failure when multiple network cards\n",
    "#os.environ['SPARK_LOCAL_IP'] = '192.168.56.1'\n",
    "os.environ['HADOOP_HOME'] = \"D:/hadoop-3.3.0\"\n",
    "os.environ['HADOOP_CONF_DIR'] = \"/usr/local/hadoop/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf  = pyspark.SparkConf().setAppName('BikeSharing1').setMaster('spark://192.168.133.4:7077').set(\n",
    "    \"spark.submit.deployMode\",\"client\").set('spark.driver.memory','6g').set(\n",
    "        'spark.executor.memory', '2g').set('spark.executor.cores', 1).set(\n",
    "        'spark.network.timeout', 600).set('spark.executor.heartbeatInterval', 120).set(\n",
    "    'spark.cores.max', 4).set(\"spark.driver.host\",\"192.168.133.1\").set(\"spark.driver.port\",\"9999\").set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")#.set('spark.python.profile','true')\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs://192.168.133.4:9000/user/hduser/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      "\n",
      "None\n",
      "tvs_pipeline\n",
      "RMSE: 82.33040601248787\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|22.0|              13.0|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364|0.47|   0.3284|59.0|              20.5|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364| 0.8|   0.2985|52.0|              20.5|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0| 0.3|0.2879|0.33|   0.2239|96.0|56.583333333333336|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0|56.583333333333336|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "dt_cv_pipeline\n",
      "RMSE: 81.3382702476952\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|22.0| 18.77777777777778|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364|0.47|   0.3284|59.0| 18.77777777777778|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364| 0.8|   0.2985|52.0| 18.77777777777778|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0| 0.3|0.2879|0.33|   0.2239|96.0|56.583333333333336|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0|56.583333333333336|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "dt_pipeline\n",
      "RMSE: 98.88683730707152\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+-----------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|       prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+-----------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|22.0|57.13194444444444|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364|0.47|   0.3284|59.0|57.13194444444444|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364| 0.8|   0.2985|52.0|57.13194444444444|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0| 0.3|0.2879|0.33|   0.2239|96.0|57.13194444444444|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0|57.13194444444444|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "gbt_pipeline\n",
      "RMSE: 81.3382702476952\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|22.0| 18.77777777777778|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364|0.47|   0.3284|59.0| 18.77777777777778|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364| 0.8|   0.2985|52.0| 18.77777777777778|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0| 0.3|0.2879|0.33|   0.2239|96.0|56.583333333333336|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0|56.583333333333336|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "gbt_cv_pipeline\n",
      "RMSE: 78.40438489328209\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|22.0|47.934736293375444|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364|0.47|   0.3284|59.0| 46.02782756554095|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1364| 0.8|   0.2985|52.0| 51.84035387126775|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0| 0.3|0.2879|0.33|   0.2239|96.0| 51.21647262030325|\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.38|0.3939| 0.4|   0.2836|91.0| 51.63558934004724|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 78.40438489328209)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class bike_training():\n",
    "\n",
    "    def __init__(self,path):\n",
    "        \n",
    "        # read csv file by sparksql\n",
    "        self.df = spark.read.format('csv')\\\n",
    "            .option('header','true').load(path+\"data/bikes/hour.csv\")\n",
    "\n",
    "        # define the evaluation of the model \n",
    "        self.evaluator = RegressionEvaluator(labelCol = 'cnt',\n",
    "                               predictionCol ='prediction',\n",
    "                               metricName = 'rmse')\n",
    "        \n",
    "        \n",
    "    def preprocessing(self,split_ratio):\n",
    "        self.hour_df = self.df.drop(\"instant\").drop(\"dteday\").drop('yr').drop('casual').drop('registered')\n",
    "        self.hour_df = self.hour_df.select([col(column).cast(\"double\").alias(column)\n",
    "                     for column in self.hour_df.columns])\n",
    "        print(self.hour_df.printSchema())\n",
    "        self.train_df, self.test_df =  self.hour_df.randomSplit(split_ratio)\n",
    "        \n",
    "        # to make read faster cache the df in the memory\n",
    "        self.train_df.cache()\n",
    "        self.test_df.cache()\n",
    "        \n",
    "    \n",
    "    def pipelines(self):\n",
    "        \n",
    "        ## pipeline common steps\n",
    "        featuresCols = self.hour_df.columns[:-1]\n",
    "        vectorAssembler = VectorAssembler(inputCols = featuresCols, outputCol='aFeatures')\n",
    "        vecotrIndexer = VectorIndexer(inputCol='aFeatures',outputCol='features',maxCategories=24)\n",
    "        \n",
    "        # Decision Tree models\n",
    "        dt = DecisionTreeRegressor(labelCol=\"cnt\",featuresCol=\"features\")\n",
    "        self.dt_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,dt])\n",
    "        \n",
    "        # Gradient Boost models\n",
    "        gbt = GBTRegressor(labelCol='cnt',featuresCol='features')\n",
    "        self.gbt_cv_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,gbt])\n",
    "        \n",
    "        \n",
    "        # ModelTuning parameters\n",
    "        self.paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(dt.maxDepth,[5,10,15,25])\\\n",
    "            .addGrid(dt.maxBins,[25,35,45,50])\\\n",
    "            .build()\n",
    "\n",
    "        # Decision Tree Cross-Validation        \n",
    "        cv = CrossValidator(estimator=dt,evaluator=self.evaluator,\n",
    "                estimatorParamMaps=self.paramGrid)\n",
    "        self.cv_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,cv])\n",
    "        \n",
    "        # Decision Tree Train-Validation\n",
    "        tvs = TrainValidationSplit(estimator=dt,evaluator=self.evaluator,\n",
    "                          estimatorParamMaps=self.paramGrid)\n",
    "        self.tvs_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,tvs])\n",
    "        \n",
    "        # Gradient Boost Cross-Validation\n",
    "        gbt_cv = CrossValidator(estimator=dt,evaluator=self.evaluator,\n",
    "        estimatorParamMaps=self.paramGrid)\n",
    "        self.gbt_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,gbt_cv])\n",
    "        \n",
    "    def dt_pipeline_ModelTraining(self,pipeline):\n",
    "        \n",
    "        # Training the pipeline\n",
    "        dt_pipeline_Model  = pipeline.fit(self.train_df)\n",
    "        \n",
    "        # make a prediction by the model\n",
    "        predicted_df = dt_pipeline_Model.transform(self.test_df)\n",
    "        \n",
    "        # Root mean square of the model\n",
    "        rmse = self.evaluator.evaluate(predicted_df)\n",
    "        print('RMSE: {}'.format(rmse))\n",
    "        return predicted_df.drop('aFeatures').drop('features').show(5),rmse\n",
    "        \n",
    "bikeModel = bike_training(path)\n",
    "bikeModel.preprocessing([0.7,0.3])\n",
    "bikeModel.pipelines()\n",
    "print(\"tvs_pipeline\")\n",
    "bikeModel.dt_pipeline_ModelTraining(bikeModel.tvs_pipeline)\n",
    "print(\"dt_cv_pipeline\")\n",
    "bikeModel.dt_pipeline_ModelTraining(bikeModel.cv_pipeline)\n",
    "print(\"dt_pipeline\")\n",
    "bikeModel.dt_pipeline_ModelTraining(bikeModel.dt_pipeline)\n",
    "print(\"gbt_pipeline\")\n",
    "bikeModel.dt_pipeline_ModelTraining(bikeModel.gbt_pipeline)\n",
    "print(\"gbt_cv_pipeline\")\n",
    "bikeModel.dt_pipeline_ModelTraining(bikeModel.gbt_cv_pipeline)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17379"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour_df = spark.read.format('csv')\\\n",
    "            .option('header','true').load(path+\"data/bikes/hour.csv\")\n",
    "hour_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df = hour_df.drop(\"instant\").drop(\"dteday\").drop('yr').drop('casual').drop('registered')\n",
    "print(hour_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_df = hour_df.select([col(column).cast(\"double\").alias(column)\n",
    "                             for column in hour_df.columns])\n",
    "hour_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df =  hour_df.randomSplit([0.8,0.2])\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol = 'cnt',\n",
    "                               predictionCol ='prediction',\n",
    "                               metricName = 'rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols = hour_df.columns[:-1]\n",
    "print(featureCols)\n",
    "vectorAssembler = VectorAssembler(inputCols = featuresCols, outputCol='aFeatures')\n",
    "vecotrIndexer = VectorIndexer(inputCol='aFeatures',outputCol='features',maxCategories=24)\n",
    "dt = DecisionTreeRegressor(labelCol=\"cnt\",featuresCol=\"features\")\n",
    "dt_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipeline_Model = dt_pipeline.fit(train_df)\n",
    "print(dt_pipeline_Model.stages[2].toDebugString[:500])\n",
    "predicted_df = dt_pipeline_Model.transform(test_df)\n",
    "predicted_df.drop('aFeatures').drop('features').show(10)\n",
    "predicted_df = dt_pipeline_Model.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(dt.maxDepth,[5,10,15,25])\\\n",
    "            .addGrid(dt.maxBins,[25,35,45,50])\\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=dt,evaluator=evaluator,\n",
    "                          estimatorParamMaps=paramGrid)\n",
    "tvs_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,tvs])\n",
    "tvs_pipelineModel = tvs_pipeline.fit(train_df)\n",
    "predicted_df = tvs_pipelineModel.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt,evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid)\n",
    "cv_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,cv])\n",
    "cv_pipelineModel = cv_pipeline.fit(train_df)\n",
    "predicted_df = cv_pipelineModel.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol='cnt',featuresCol='features')\n",
    "gbt_pipeline = Pipeline(stages=[vectorAssembler, vecotrIndexer,gbt])\n",
    "gbt_pipelineModel = gbt_pipeline.fit(train_df)\n",
    "predicted_df = gbt_pipelineModel.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.drop('aFeatures').drop('features').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
