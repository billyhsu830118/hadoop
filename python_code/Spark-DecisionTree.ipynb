{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pywebhdfs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "display(salaries.head())\n",
    "display(teams.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://pythonhosted.org/pywebhdfs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import os\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/home/hduser/anaconda3/bin/python'\n",
    "# The value is the python command of the version required to start the master and worker in the Linux system\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\billy\\anaconda3\\python.exe\"\n",
    "# The value is the spark directory in the local windows system\n",
    "os.environ['SPARK_HOME'] = 'C:/spark'\n",
    "# The value is the local IP, and the IP required to establish a connection, to prevent connection failure when multiple network cards\n",
    "#os.environ['SPARK_LOCAL_IP'] = '192.168.56.1'\n",
    "os.environ['HADOOP_HOME'] = \"D:/hadoop-3.3.0\"\n",
    "os.environ['HADOOP_CONF_DIR'] = \"/usr/local/hadoop/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf  = pyspark.SparkConf().setAppName('forest').setMaster('spark://192.168.133.4:7077').set(\n",
    "    \"spark.submit.deployMode\",\"client\").set('spark.driver.memory','2g').set(\n",
    "        'spark.executor.memory', '2g').set('spark.executor.cores', 1).set(\n",
    "        'spark.network.timeout', 600).set('spark.executor.heartbeatInterval', 120).set(\n",
    "    'spark.cores.max', 4).set(\"spark.driver.host\",\"192.168.133.1\").set(\"spark.driver.port\",\"9999\").set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")#.set('spark.python.profile','true')\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs://192.168.133.4:9000/user/hduser/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581012\n"
     ]
    }
   ],
   "source": [
    "RawUserRDD = sc.textFile(path+\"data/covtype.data\")\n",
    "print(RawUserRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2596', '51', '3', '258', '0', '510', '221', '232', '148', '6279', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '5']]\n"
     ]
    }
   ],
   "source": [
    "RawData = RawUserRDD.map(lambda x:x.split(\",\"))\n",
    "print(RawData.take(1),sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_row:581012\n",
      "trainData:464590, validationData:58351, testData:58071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainData, validationData, testData) = PrepareData(sc)\n",
    "trainData.cache()\n",
    "validationData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "model = DecisionTree.trainClassifier(trainData,\n",
    "                    numClasses=7, categoricalFeaturesInfo={},\n",
    "                    impurity=\"entropy\",\n",
    "                    maxDepth=8,\n",
    "                    maxBins=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluateModel(model, validationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(sc):\n",
    "    rawData = sc.textFile(path+\"data/covtype.data\")\n",
    "    print('total_row:{}'.format(rawData.count() ))\n",
    "    lines = rawData.map(lambda x:x.split(\",\"))\n",
    "    labelpointRDD = lines.map(lambda r:LabeledPoint(\n",
    "                            extract_label(r),\n",
    "                            extract_features(r,len(r)-1)))\n",
    "    (trainData, validationData, testData) = labelpointRDD.randomSplit([8, 1, 1])\n",
    "    print('trainData:{}, validationData:{}, testData:{}'.format(\n",
    "                trainData.count(),validationData.count(),testData.count()))\n",
    "    return (trainData, validationData, testData)\n",
    "    \n",
    "def extract_label(record):\n",
    "    label = (record[-1])\n",
    "    return float(label)-1\n",
    "\n",
    "def extract_features(record,featureEnd):\n",
    "    numericalFeatures = [convert_float(field) for field in record[0: featureEnd]]\n",
    "    return numericalFeatures\n",
    "\n",
    "def convert_float(x):\n",
    "    return (0 if x==\"?\" else float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEvaluateModel(trainData, validationData, impurityParm, maxDepthParm, maxBinsParm):\n",
    "    model = DecisionTree.trainClassifier(trainData,\n",
    "                        numClasses=7, categoricalFeaturesInfo={},\n",
    "                        impurity=impurityParm,\n",
    "                        maxDepth=maxDepthParm,\n",
    "                        maxBins=maxBinsParm)\n",
    "    accuracy = evaluateModel(model, validationData)\n",
    "\n",
    "    return (accuracy,impurityParm,maxDepthParm,maxBinsParm,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, validationData):\n",
    "    score = model.predict(validationData.map(lambda p:p.features))\n",
    "    scoreAndLabels = score.zip(validationData.map(lambda p:p.label))\n",
    "    metrics = MulticlassMetrics(scoreAndLabels)\n",
    "    accuracy = metrics.accuracy\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Method __init__ forces keyword arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7606d94608c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategoricalFeaturesInfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimpurity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'variance'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparamGrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParamGridBuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxDepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxBins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m36\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Method __init__ forces keyword arguments."
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(trainData, categoricalFeaturesInfo={}, impurity='variance', numClasses=2)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, [4, 5, 6, 7]) \\\n",
    "    .addGrid(model.maxBins, [24, 28, 32, 36]) \\\n",
    "    .build()\n",
    "\n",
    "# crossval = CrossValidator(estimator=model,\n",
    "#                           estimatorParamMaps=paramGrid,\n",
    "#                           evaluator=BinaryClassificationEvaluator(),\n",
    "#                           numFolds=3)  \n",
    "\n",
    "# # Run cross-validation, and choose the best set of parameters.\n",
    "# cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = DecisionTree.trainClassifier(trainData, categoricalFeaturesInfo={},  numClasses=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "param must be an instance of Param",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9341adf5d059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# the evaluator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mparamGrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParamGridBuilder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36maddGrid\u001b[1;34m(self, param, values)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"param must be an instance of Param\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: param must be an instance of Param"
     ]
    }
   ],
   "source": [
    "model  = DecisionTree.trainClassifier(trainData, categoricalFeaturesInfo={},  numClasses=7)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.depth, [4, 5, 6, 7]) \\\n",
    "    .build()\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = CrossValidator(estimator=DTree,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8,\n",
    "                    numFolds = 5)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(trainData)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(testData)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labelpointRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b411abdc62c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabelpointRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"correct\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labelpointRDD' is not defined"
     ]
    }
   ],
   "source": [
    "for lp in labelpointRDD.take(100):\n",
    "    predict = model.predict(lp.features)\n",
    "    label = lp.label\n",
    "    features = lp.features\n",
    "    result = (\"correct\" if (label ==predict) else \"error\")\n",
    "    print(\"土地條件:海拔:\" + str(features[0])+\n",
    "         \" 方位: \" + str(features[1]) +\n",
    "         \" 斜率: \" + str(features[2]) +\n",
    "         \" 水源垂直距離: \" + str(features[3]) +\n",
    "         \" 水源水平距離: \" + str(features[4]) +\n",
    "         \" 9點時陰影: \" + str(features[5]) +\n",
    "         \" ...==>預測: \" + str(predict) +\n",
    "         \" 實際: \" + str(label)+\"結果:\"+result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,StructField,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581012"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = sc.textFile(path+'data/covtype.data')\n",
    "lines = rawData.map(lambda x:x.split(\",\"))\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_num = len(lines.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField(\"field\"+str(i),StringType(),True)\n",
    "          for i in range(field_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(field0,StringType,true),StructField(field1,StringType,true),StructField(field2,StringType,true),StructField(field3,StringType,true),StructField(field4,StringType,true),StructField(field5,StringType,true),StructField(field6,StringType,true),StructField(field7,StringType,true),StructField(field8,StringType,true),StructField(field9,StringType,true),StructField(field10,StringType,true),StructField(field11,StringType,true),StructField(field12,StringType,true),StructField(field13,StringType,true),StructField(field14,StringType,true),StructField(field15,StringType,true),StructField(field16,StringType,true),StructField(field17,StringType,true),StructField(field18,StringType,true),StructField(field19,StringType,true),StructField(field20,StringType,true),StructField(field21,StringType,true),StructField(field22,StringType,true),StructField(field23,StringType,true),StructField(field24,StringType,true),StructField(field25,StringType,true),StructField(field26,StringType,true),StructField(field27,StringType,true),StructField(field28,StringType,true),StructField(field29,StringType,true),StructField(field30,StringType,true),StructField(field31,StringType,true),StructField(field32,StringType,true),StructField(field33,StringType,true),StructField(field34,StringType,true),StructField(field35,StringType,true),StructField(field36,StringType,true),StructField(field37,StringType,true),StructField(field38,StringType,true),StructField(field39,StringType,true),StructField(field40,StringType,true),StructField(field41,StringType,true),StructField(field42,StringType,true),StructField(field43,StringType,true),StructField(field44,StringType,true),StructField(field45,StringType,true),StructField(field46,StringType,true),StructField(field47,StringType,true),StructField(field48,StringType,true),StructField(field49,StringType,true),StructField(field50,StringType,true),StructField(field51,StringType,true),StructField(field52,StringType,true),StructField(field53,StringType,true),StructField(field54,StringType,true)))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x000001B3242DE4C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\billy\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\pyspark\\ml\\wrapper.py\", line 42, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'DecisionTreeClassifier' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "covtype_df = spark.createDataFrame(lines,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "covtype_df = covtype_df.select([ col(column).cast(\"double\").alias(column)\n",
    "                                for column in covtype_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field0: double (nullable = true)\n",
      " |-- field1: double (nullable = true)\n",
      " |-- field2: double (nullable = true)\n",
      " |-- field3: double (nullable = true)\n",
      " |-- field4: double (nullable = true)\n",
      " |-- field5: double (nullable = true)\n",
      " |-- field6: double (nullable = true)\n",
      " |-- field7: double (nullable = true)\n",
      " |-- field8: double (nullable = true)\n",
      " |-- field9: double (nullable = true)\n",
      " |-- field10: double (nullable = true)\n",
      " |-- field11: double (nullable = true)\n",
      " |-- field12: double (nullable = true)\n",
      " |-- field13: double (nullable = true)\n",
      " |-- field14: double (nullable = true)\n",
      " |-- field15: double (nullable = true)\n",
      " |-- field16: double (nullable = true)\n",
      " |-- field17: double (nullable = true)\n",
      " |-- field18: double (nullable = true)\n",
      " |-- field19: double (nullable = true)\n",
      " |-- field20: double (nullable = true)\n",
      " |-- field21: double (nullable = true)\n",
      " |-- field22: double (nullable = true)\n",
      " |-- field23: double (nullable = true)\n",
      " |-- field24: double (nullable = true)\n",
      " |-- field25: double (nullable = true)\n",
      " |-- field26: double (nullable = true)\n",
      " |-- field27: double (nullable = true)\n",
      " |-- field28: double (nullable = true)\n",
      " |-- field29: double (nullable = true)\n",
      " |-- field30: double (nullable = true)\n",
      " |-- field31: double (nullable = true)\n",
      " |-- field32: double (nullable = true)\n",
      " |-- field33: double (nullable = true)\n",
      " |-- field34: double (nullable = true)\n",
      " |-- field35: double (nullable = true)\n",
      " |-- field36: double (nullable = true)\n",
      " |-- field37: double (nullable = true)\n",
      " |-- field38: double (nullable = true)\n",
      " |-- field39: double (nullable = true)\n",
      " |-- field40: double (nullable = true)\n",
      " |-- field41: double (nullable = true)\n",
      " |-- field42: double (nullable = true)\n",
      " |-- field43: double (nullable = true)\n",
      " |-- field44: double (nullable = true)\n",
      " |-- field45: double (nullable = true)\n",
      " |-- field46: double (nullable = true)\n",
      " |-- field47: double (nullable = true)\n",
      " |-- field48: double (nullable = true)\n",
      " |-- field49: double (nullable = true)\n",
      " |-- field50: double (nullable = true)\n",
      " |-- field51: double (nullable = true)\n",
      " |-- field52: double (nullable = true)\n",
      " |-- field53: double (nullable = true)\n",
      " |-- field54: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covtype_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9', 'field10', 'field11', 'field12', 'field13', 'field14', 'field15', 'field16', 'field17', 'field18', 'field19', 'field20', 'field21', 'field22', 'field23', 'field24', 'field25', 'field26', 'field27', 'field28', 'field29', 'field30', 'field31', 'field32', 'field33', 'field34', 'field35', 'field36', 'field37', 'field38', 'field39', 'field40', 'field41', 'field42', 'field43', 'field44', 'field45', 'field46', 'field47', 'field48', 'field49', 'field50', 'field51', 'field52', 'field53']\n"
     ]
    }
   ],
   "source": [
    "featuresCols=covtype_df.columns[:54]\n",
    "print(featuresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_df = covtype_df.withColumn(\"label\",covtype_df[\"field54\"]-1).drop(\"field54\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-----+\n",
      "|field0|field1|field2|field3|field4|field5|field6|field7|field8|field9|field10|field11|field12|field13|field14|field15|field16|field17|field18|field19|field20|field21|field22|field23|field24|field25|field26|field27|field28|field29|field30|field31|field32|field33|field34|field35|field36|field37|field38|field39|field40|field41|field42|field43|field44|field45|field46|field47|field48|field49|field50|field51|field52|field53|label|\n",
      "+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-----+\n",
      "|2596.0|  51.0|   3.0| 258.0|   0.0| 510.0| 221.0| 232.0| 148.0|6279.0|    1.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    1.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|    0.0|  4.0|\n",
      "+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covtype_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[field0: double, field1: double, field2: double, field3: double, field4: double, field5: double, field6: double, field7: double, field8: double, field9: double, field10: double, field11: double, field12: double, field13: double, field14: double, field15: double, field16: double, field17: double, field18: double, field19: double, field20: double, field21: double, field22: double, field23: double, field24: double, field25: double, field26: double, field27: double, field28: double, field29: double, field30: double, field31: double, field32: double, field33: double, field34: double, field35: double, field36: double, field37: double, field38: double, field39: double, field40: double, field41: double, field42: double, field43: double, field44: double, field45: double, field46: double, field47: double, field48: double, field49: double, field50: double, field51: double, field52: double, field53: double, label: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[field0: double, field1: double, field2: double, field3: double, field4: double, field5: double, field6: double, field7: double, field8: double, field9: double, field10: double, field11: double, field12: double, field13: double, field14: double, field15: double, field16: double, field17: double, field18: double, field19: double, field20: double, field21: double, field22: double, field23: double, field24: double, field25: double, field26: double, field27: double, field28: double, field29: double, field30: double, field31: double, field32: double, field33: double, field34: double, field35: double, field36: double, field37: double, field38: double, field39: double, field40: double, field41: double, field42: double, field43: double, field44: double, field45: double, field46: double, field47: double, field48: double, field49: double, field50: double, field51: double, field52: double, field53: double, label: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = covtype_df.randomSplit([0.7,0.3])\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=featuresCols,\n",
    "                                 outputCol = \"features\")\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\",featuresCol=\"features\",\n",
    "                           maxDepth=5,maxBins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipeline = Pipeline(stages=[vectorAssembler,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_c081158e6aec, DecisionTreeClassifier_bf7b19909585]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = dt_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['field0', 'field1', 'field2', 'field3', 'field4', 'field5', 'field6', 'field7', 'field8', 'field9', 'field10', 'field11', 'field12', 'field13', 'field14', 'field15', 'field16', 'field17', 'field18', 'field19', 'field20', 'field21', 'field22', 'field23', 'field24', 'field25', 'field26', 'field27', 'field28', 'field29', 'field30', 'field31', 'field32', 'field33', 'field34', 'field35', 'field36', 'field37', 'field38', 'field39', 'field40', 'field41', 'field42', 'field43', 'field44', 'field45', 'field46', 'field47', 'field48', 'field49', 'field50', 'field51', 'field52', 'field53', 'label', 'features', 'rawPrediction', 'probability', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "print(predicted.columns,sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|[0.0,324.0,10268....|[0.0,0.0206527281...|       2.0|\n",
      "|(54,[0,1,2,5,6,7,...|[0.0,31.0,395.0,7...|[0.0,0.0234848484...|       3.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.select('features', 'rawPrediction', 'probability', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"label\",predictionCol=\"prediction\",\n",
    "                metricName=\"accuracy\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7019220862477533"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipelineModel.transform(test_df)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(dt.impurity,['gini','entropy'])\\\n",
    "        .addGrid(dt.maxDepth, [10,15,25])\\\n",
    "        .addGrid(dt.maxBins,[30,40,50])\\\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator = dt, evaluator= evaluator,\n",
    "                          estimatorParamMaps=paramGrid,trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs_pipeline = Pipeline(stages = [vectorAssembler,tvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs_pipelineModel = tvs_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_bf7b19909585, depth=25, numNodes=44125, numClasses=7, numFeatures=54\n",
      "  If (feature 0 <= 2693.5)\n",
      "   If (feature 0 <= 2512.5)\n",
      "    If (feature 0 <= 2413.5)\n",
      "     If (feature 23 <= 0.5)\n",
      "      If (feature 3 <= 15.0)\n",
      "       If (feature 13 <= 0.5)\n",
      "        If (feature 9 <= 574.5)\n",
      "         If (feature 16 <= 0.5)\n",
      "          Predict: 5.0\n",
      "         Else (feature 16 > 0.5)\n",
      "          If (feature 5 <= 380.5)\n",
      "           Predict: 5.0\n",
      "          Else (featur\n"
     ]
    }
   ],
   "source": [
    "bestModel = tvs_pipelineModel.stages[1].bestModel\n",
    "print(bestModel.toDebugString[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+--------+--------+-----+-----+----------+\n",
      "|  海拔| 方位|斜率|垂直距離|水平距離| 陰影|label|prediction|\n",
      "+------+-----+----+--------+--------+-----+-----+----------+\n",
      "|1861.0| 35.0|14.0|    60.0|    11.0| 85.0|  2.0|       2.0|\n",
      "|1866.0| 23.0|14.0|    85.0|    16.0|108.0|  2.0|       2.0|\n",
      "|1872.0| 27.0|21.0|   108.0|    30.0| 67.0|  5.0|       5.0|\n",
      "|1876.0| 25.0|17.0|   124.0|    26.0|150.0|  2.0|       2.0|\n",
      "|1876.0| 29.0|19.0|   124.0|    34.0| 90.0|  5.0|       5.0|\n",
      "|1877.0| 28.0|22.0|   127.0|    35.0| 85.0|  5.0|       5.0|\n",
      "|1880.0| 35.0|21.0|   150.0|    32.0|150.0|  2.0|       2.0|\n",
      "|1880.0| 38.0|21.0|   150.0|    38.0|120.0|  5.0|       5.0|\n",
      "|1882.0| 34.0|21.0|    85.0|    28.0|108.0|  2.0|       2.0|\n",
      "|1882.0|330.0|18.0|     0.0|     0.0|120.0|  5.0|       5.0|\n",
      "+------+-----+----+--------+--------+-----+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = tvs_pipelineModel.transform(test_df)\n",
    "result = predictions.withColumnRenamed(\"field0\",\"海拔\")\\\n",
    "                    .withColumnRenamed(\"field1\",\"方位\")\\\n",
    "                    .withColumnRenamed(\"field2\",\"斜率\")\\\n",
    "                    .withColumnRenamed(\"field3\",\"垂直距離\")\\\n",
    "                    .withColumnRenamed(\"field4\",\"水平距離\")\\\n",
    "                    .withColumnRenamed(\"field5\",\"陰影\")\n",
    "result.select(\"海拔\",\"方位\",\"斜率\",\"垂直距離\",\"水平距離\",\"陰影\",'label','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
